import os
import re
import subprocess
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from collections import Counter
from wordcloud import WordCloud

import nltk
from nltk.corpus import stopwords

nltk.download("stopwords")
stop_words = set(stopwords.words("english"))
import pdfplumber
import docx

# PDF
def extract_text_from_pdf(path):
    text = ""
    with pdfplumber.open(path) as pdf:
        for page in pdf.pages:
            text += page.extract_text() or ""
    return text

# DOCX
def extract_text_from_docx(path):
    doc = docx.Document(path)
    return " ".join(p.text for p in doc.paragraphs)

# DOC â†’ DOCX (Linux / Colab)
def convert_doc_to_docx(doc_path):
    subprocess.run([
        "libreoffice",
        "--headless",
        "--convert-to",
        "docx",
        doc_path,
        "--outdir",
        os.path.dirname(doc_path)
    ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
base_path = "/content/drive/MyDrive/Resume"  # change if needed

profile = []
file_name = []
file_format = []

for folder in os.listdir(base_path):
    folder_path = os.path.join(base_path, folder)

    if os.path.isdir(folder_path):
        for file in os.listdir(folder_path):
            if file.lower().endswith((".pdf", ".docx", ".doc")):
                profile.append(folder)
                file_name.append(file)
                file_format.append(file.split(".")[-1].lower())

profile_df = pd.DataFrame({
    "Profile": profile,
    "File_Name": file_name,
    "File_Format": file_format
})

profile_df.head()

print("Profile Distribution:\n")
print(profile_df["Profile"].value_counts())

print("\nFile Format Distribution:\n")
print(profile_df["File_Format"].value_counts())

profile_df["File_Format"].value_counts().plot(
    kind="bar",
    title="File Format Distribution",
    figsize=(6,4)
)
plt.show()
data = []

for _, row in profile_df.iterrows():
    folder = row["Profile"]
    file = row["File_Name"]

    full_path = os.path.join(base_path, folder, file)

    try:
        if file.lower().endswith(".pdf"):
            text = extract_text_from_pdf(full_path)

        elif file.lower().endswith(".docx"):
            text = extract_text_from_docx(full_path)

        elif file.lower().endswith(".doc"):
            convert_doc_to_docx(full_path)
            new_path = full_path.replace(".doc", ".docx")
            text = extract_text_from_docx(new_path)

        else:
            continue

        if text.strip():
            data.append([text, folder])

    except Exception as e:
        print("Skipped:", file, "| Reason:", e)

df = pd.DataFrame(data, columns=["Resume", "Category"])
print("Loaded resumes:", df.shape)
df.head()
plt.figure(figsize=(8,5))
sns.countplot(y="Category", data=df, order=df["Category"].value_counts().index)
plt.title("Resume Category Distribution")
plt.show()

X = df["Clean_Resume"]   # input features (text)
y = df["Category"]  

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X,
    y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
lr_pipeline = Pipeline([
    ("tfidf", TfidfVectorizer(
        max_features=3000,
        ngram_range=(1, 2)
    )),
    ("lr", LogisticRegression(max_iter=1000))
from sklearn.metrics import accuracy_score, classification_report
lr_pipeline.fit(X_train, y_train)

y_pred_lr = lr_pipeline.predict(X_test)
accuracy_lr=accuracy_score(y_test, y_pred_lr)
print("Logistic Regression Accuracy:", accuracy_score(y_test, y_pred_lr))
print("\nLogistic Regression Classification Report:\n")
print(classification_report(y_test, y_pred_lr))
from sklearn.naive_bayes import MultinomialNB
nb_pipeline = Pipeline([
    ("tfidf", TfidfVectorizer(
        max_features=3000,
        ngram_range=(1, 2)
    )),
    ("nb", MultinomialNB())
])

nb_pipeline.fit(X_train, y_train)

y_pred_nb = nb_pipeline.predict(X_test)
accuracy_nb=accuracy_score(y_test, y_pred_nb)
print("Naive Bayes Accuracy:", accuracy_score(y_test, y_pred_nb))
print("\nNaive Bayes Classification Report:\n")
print(classification_report(y_test, y_pred_nb))
from sklearn.svm import LinearSVC
svm_pipeline = Pipeline([
    ("tfidf", TfidfVectorizer(
        max_features=3000,
        ngram_range=(1, 2)
    )),
    ("svm", LinearSVC())
])
svm_pipeline.fit(X_train, y_train)

y_pred_svm = svm_pipeline.predict(X_test)
accuracy_svm=accuracy_score(y_test, y_pred_svm)
print("SVM Accuracy:", accuracy_score(y_test, y_pred_svm))
print("\nSVM Classification Report:\n")
print(classification_report(y_test, y_pred_svm))
def predict_resume_category(resume_text):
    cleaned = clean_text(resume_text)
    return svm_pipeline.predict([cleaned])[0]
def predict_resume_category_from_file(file_path):

    if file_path.lower().endswith(".pdf"):
        text = extract_text_from_pdf(file_path)

    elif file_path.lower().endswith(".docx"):
        text = extract_text_from_docx(file_path)

    elif file_path.lower().endswith(".doc"):
        convert_doc_to_docx(file_path)
        new_path = file_path.replace(".doc", ".docx")
        text = extract_text_from_docx(new_path)

    else:
        return "Unsupported file format"

    if not text.strip():
        return "Empty or invalid resume"

    return predict_resume_category(text)
predict_resume_category_from_file(
    "/content/drive/MyDrive/Resume/React/React Dev_Krishna Kanth.docx"
)
import pickle

with open("svm_pipeline.pkl", "wb") as f:
    pickle.dump(svm_pipeline, f)



])
